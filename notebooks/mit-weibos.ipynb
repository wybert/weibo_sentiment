{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef6621c7453422aac14943047b9663c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical_Geotagged_Data_7.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5425/2568029488.py:18: DtypeWarning: Columns (0,1,12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "/mnt/c/Users/xif626/Documents/GitHub/weibo_sentiment/weibo_sentiment/prepare/clean_data.py:136: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # sample['text_clean'] = sample.progress_apply(clean_a_test,axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use  0:00:44.674388\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "(\"Could not convert '2326098915' with type str: tried to convert to int64\", 'Conversion failed for column userid with type object')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(e)\n\u001b[1;32m     23\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m df\u001b[39m.\u001b[39;49mto_parquet(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(pa_path, file_name\u001b[39m.\u001b[39;49mreplace(\u001b[39m\"\u001b[39;49m\u001b[39m.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m.parquet\u001b[39;49m\u001b[39m\"\u001b[39;49m)))\n\u001b[1;32m     25\u001b[0m \u001b[39m# print(df.shape)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m# break\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39m# df.to_csv(file_path, sep='\\t', header=None, index=False) \u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/site-packages/pandas/core/frame.py:2976\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2889\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2890\u001b[0m \u001b[39mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2891\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2972\u001b[0m \u001b[39m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2973\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   2974\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mparquet\u001b[39;00m \u001b[39mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2976\u001b[0m \u001b[39mreturn\u001b[39;00m to_parquet(\n\u001b[1;32m   2977\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   2978\u001b[0m     path,\n\u001b[1;32m   2979\u001b[0m     engine,\n\u001b[1;32m   2980\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   2981\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m   2982\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m   2983\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   2984\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2985\u001b[0m )\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/site-packages/pandas/io/parquet.py:430\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m impl \u001b[39m=\u001b[39m get_engine(engine)\n\u001b[1;32m    428\u001b[0m path_or_buf: FilePath \u001b[39m|\u001b[39m WriteBuffer[\u001b[39mbytes\u001b[39m] \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m path\n\u001b[0;32m--> 430\u001b[0m impl\u001b[39m.\u001b[39;49mwrite(\n\u001b[1;32m    431\u001b[0m     df,\n\u001b[1;32m    432\u001b[0m     path_or_buf,\n\u001b[1;32m    433\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    434\u001b[0m     index\u001b[39m=\u001b[39;49mindex,\n\u001b[1;32m    435\u001b[0m     partition_cols\u001b[39m=\u001b[39;49mpartition_cols,\n\u001b[1;32m    436\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m    437\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    438\u001b[0m )\n\u001b[1;32m    440\u001b[0m \u001b[39mif\u001b[39;00m path \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, io\u001b[39m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/site-packages/pandas/io/parquet.py:174\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m index \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    172\u001b[0m     from_pandas_kwargs[\u001b[39m\"\u001b[39m\u001b[39mpreserve_index\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m index\n\u001b[0;32m--> 174\u001b[0m table \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapi\u001b[39m.\u001b[39;49mTable\u001b[39m.\u001b[39;49mfrom_pandas(df, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfrom_pandas_kwargs)\n\u001b[1;32m    176\u001b[0m path_or_handle, handles, kwargs[\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m _get_path_or_handle(\n\u001b[1;32m    177\u001b[0m     path,\n\u001b[1;32m    178\u001b[0m     kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mfilesystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m     is_dir\u001b[39m=\u001b[39mpartition_cols \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    182\u001b[0m )\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    184\u001b[0m     \u001b[39misinstance\u001b[39m(path_or_handle, io\u001b[39m.\u001b[39mBufferedWriter)\n\u001b[1;32m    185\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(path_or_handle, \u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_handle\u001b[39m.\u001b[39mname, (\u001b[39mstr\u001b[39m, \u001b[39mbytes\u001b[39m))\n\u001b[1;32m    187\u001b[0m ):\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/site-packages/pyarrow/table.pxi:3557\u001b[0m, in \u001b[0;36mpyarrow.lib.Table.from_pandas\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/site-packages/pyarrow/pandas_compat.py:624\u001b[0m, in \u001b[0;36mdataframe_to_arrays\u001b[0;34m(df, schema, preserve_index, nthreads, columns, safe)\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[39mfor\u001b[39;00m i, maybe_fut \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(arrays):\n\u001b[1;32m    623\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_fut, futures\u001b[39m.\u001b[39mFuture):\n\u001b[0;32m--> 624\u001b[0m             arrays[i] \u001b[39m=\u001b[39m maybe_fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    626\u001b[0m types \u001b[39m=\u001b[39m [x\u001b[39m.\u001b[39mtype \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arrays]\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m schema \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/concurrent/futures/_base.py:437\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    436\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/concurrent/futures/thread.py:57\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 57\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfn(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs)\n\u001b[1;32m     58\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     59\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/site-packages/pyarrow/pandas_compat.py:598\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mexcept\u001b[39;00m (pa\u001b[39m.\u001b[39mArrowInvalid,\n\u001b[1;32m    594\u001b[0m         pa\u001b[39m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    595\u001b[0m         pa\u001b[39m.\u001b[39mArrowTypeError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    596\u001b[0m     e\u001b[39m.\u001b[39margs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mConversion failed for column \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    597\u001b[0m                \u001b[39m.\u001b[39mformat(col\u001b[39m.\u001b[39mname, col\u001b[39m.\u001b[39mdtype),)\n\u001b[0;32m--> 598\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m field_nullable \u001b[39mand\u001b[39;00m result\u001b[39m.\u001b[39mnull_count \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    600\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mField \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m was non-nullable but pandas column \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    601\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mhad \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m null values\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mstr\u001b[39m(field),\n\u001b[1;32m    602\u001b[0m                                                  result\u001b[39m.\u001b[39mnull_count))\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/site-packages/pyarrow/pandas_compat.py:592\u001b[0m, in \u001b[0;36mdataframe_to_arrays.<locals>.convert_column\u001b[0;34m(col, field)\u001b[0m\n\u001b[1;32m    589\u001b[0m     type_ \u001b[39m=\u001b[39m field\u001b[39m.\u001b[39mtype\n\u001b[1;32m    591\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 592\u001b[0m     result \u001b[39m=\u001b[39m pa\u001b[39m.\u001b[39;49marray(col, \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49mtype_, from_pandas\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, safe\u001b[39m=\u001b[39;49msafe)\n\u001b[1;32m    593\u001b[0m \u001b[39mexcept\u001b[39;00m (pa\u001b[39m.\u001b[39mArrowInvalid,\n\u001b[1;32m    594\u001b[0m         pa\u001b[39m.\u001b[39mArrowNotImplementedError,\n\u001b[1;32m    595\u001b[0m         pa\u001b[39m.\u001b[39mArrowTypeError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    596\u001b[0m     e\u001b[39m.\u001b[39margs \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mConversion failed for column \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m with type \u001b[39m\u001b[39m{!s}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    597\u001b[0m                \u001b[39m.\u001b[39mformat(col\u001b[39m.\u001b[39mname, col\u001b[39m.\u001b[39mdtype),)\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/site-packages/pyarrow/array.pxi:316\u001b[0m, in \u001b[0;36mpyarrow.lib.array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/site-packages/pyarrow/array.pxi:83\u001b[0m, in \u001b[0;36mpyarrow.lib._ndarray_to_array\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/mambaforge/envs/weibo_process/lib/python3.8/site-packages/pyarrow/error.pxi:100\u001b[0m, in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mArrowInvalid\u001b[0m: (\"Could not convert '2326098915' with type str: tried to convert to int64\", 'Conversion failed for column userid with type object')"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from weibo_sentiment.prepare.clean_data import clean_text_batch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "data_path = \"/mnt/g/weibo_data/Weibo\"\n",
    "pa_path = \"/mnt/g/weibo_data/weibo_mit_parquet\"\n",
    "\n",
    "already_processed = list(os.listdir(pa_path))\n",
    "\n",
    "for file_name in tqdm(list(os.listdir(data_path))):\n",
    "    file_path = os.path.join(data_path, file_name)\n",
    "    if file_name.replace(\".csv\", \".parquet\")  in already_processed:\n",
    "        continue\n",
    "    print(file_name)\n",
    "    if os.path.isfile(file_path):\n",
    "        # print(file_path)\n",
    "        df = pd.read_csv(file_path)\n",
    "        try:\n",
    "            df = clean_text_batch(df)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "        df.to_parquet(os.path.join(pa_path, file_name.replace(\".csv\", \".parquet\")))\n",
    "        # print(df.shape)\n",
    "        # break\n",
    "        # df.to_csv(file_path, sep='\\t', header=None, index=False) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這個文件不知道爲什麽保存失敗，我後面把它轉存csv然後在轉存parquet就好了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"/mnt/g/weibo_data/weibo_mit_parquet\" + file_name, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5425/1594139801.py:1: DtypeWarning: Columns (13) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_1 = pd.read_csv(\"/mnt/g/weibo_data/weibo_mit_parquet\" + file_name,\n"
     ]
    }
   ],
   "source": [
    "df_1 = pd.read_csv(\"/mnt/g/weibo_data/weibo_mit_parquet\" + file_name, \n",
    "                   encoding=\"utf-8\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_parquet(os.path.join(pa_path, file_name.replace(\".csv\", \".parquet\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/xif626/Documents/GitHub/weibo_sentiment/weibo_sentiment/bert_senta\n"
     ]
    }
   ],
   "source": [
    "cd ../weibo_sentiment/bert_senta/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/xif626/Documents/GitHub/weibo_sentiment/weibo_sentiment/bert_senta'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/06/2023 19:01:05 - INFO - __main__ -   device cuda n_gpu 1 distributed training False\n",
      "02/06/2023 19:01:05 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.\n",
      "02/06/2023 19:01:05 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /home/kang/.pytorch_pretrained_bert/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Init model started.\n",
      "using cuda Quadro P2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/06/2023 19:01:05 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at /home/kang/.pytorch_pretrained_bert/distributed_-1/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "02/06/2023 19:01:05 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/kang/.pytorch_pretrained_bert/distributed_-1/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmpzccx1syv\n",
      "02/06/2023 19:01:07 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "02/06/2023 19:01:09 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "02/06/2023 19:01:09 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Init model finished.\n",
      "The following 10 data will be processed\n",
      "                             file_name\n",
      "0  Historical_Geotagged_Data_0.parquet\n",
      "1  Historical_Geotagged_Data_1.parquet\n",
      "2  Historical_Geotagged_Data_2.parquet\n",
      "3  Historical_Geotagged_Data_3.parquet\n",
      "4  Historical_Geotagged_Data_4.parquet\n",
      "5  Historical_Geotagged_Data_5.parquet\n",
      "6  Historical_Geotagged_Data_6.parquet\n",
      "7  Historical_Geotagged_Data_8.parquet\n",
      "8  Historical_Geotagged_Data_9.parquet\n",
      "9  Historical_Geotagged_Data_7.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8c61c2acd2141a288d2ad0f7c1382e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "total:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Historical_Geotagged_Data_0.parquet\n",
      "load samples\n",
      "load fetures\n",
      "load train data\n",
      "load tensor\n",
      "Run prediction for full data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addf6cc86cbb440391c9240c8107bec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "predicting:   0%|          | 0/1000504 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Historical_Geotagged_Data_1.parquet\n",
      "load samples\n",
      "load fetures\n",
      "load train data\n",
      "load tensor\n",
      "Run prediction for full data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ce502cbb8141aead367400f0f3cac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "predicting:   0%|          | 0/1044382 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Historical_Geotagged_Data_2.parquet\n",
      "load samples\n",
      "load fetures\n",
      "load train data\n",
      "load tensor\n",
      "Run prediction for full data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b99fa381da47aab502ce5a230922f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "predicting:   0%|          | 0/1000455 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Historical_Geotagged_Data_3.parquet\n",
      "load samples\n",
      "load fetures\n",
      "load train data\n",
      "load tensor\n",
      "Run prediction for full data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36caf1a8d73e4d86903a9ea5b1b6130c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "predicting:   0%|          | 0/1001482 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Historical_Geotagged_Data_4.parquet\n",
      "load samples\n",
      "load fetures\n",
      "load train data\n",
      "load tensor\n",
      "Run prediction for full data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc41b1a24a84f33ba2a0d36c65c83e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "predicting:   0%|          | 0/1001401 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Historical_Geotagged_Data_5.parquet\n",
      "load samples\n",
      "load fetures\n",
      "load train data\n",
      "load tensor\n",
      "Run prediction for full data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f55657453da4bd5a1e3a2f67503518d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "predicting:   0%|          | 0/1001009 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Historical_Geotagged_Data_6.parquet\n",
      "load samples\n",
      "load fetures\n",
      "load train data\n",
      "load tensor\n",
      "Run prediction for full data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7401636b26413db639b75b07fd591e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "predicting:   0%|          | 0/1000381 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Historical_Geotagged_Data_8.parquet\n",
      "load samples\n",
      "load fetures\n",
      "load train data\n",
      "load tensor\n",
      "Run prediction for full data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33fb2655d630439fb0ac34a05366c5e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "predicting:   0%|          | 0/1002774 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Historical_Geotagged_Data_9.parquet\n",
      "load samples\n",
      "load fetures\n",
      "load train data\n",
      "load tensor\n",
      "Run prediction for full data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0d8421431440cb860fdc31638612fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "predicting:   0%|          | 0/902835 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Historical_Geotagged_Data_7.parquet\n",
      "load samples\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/mnt/c/Users/xif626/Documents/GitHub/weibo_sentiment/weibo_sentiment/bert_senta/predict2.py:353\u001b[0m\n\u001b[1;32m    351\u001b[0m data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_parquet(file_path,engine\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfastparquet\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    352\u001b[0m \u001b[39m# data = data.sample(n=100,random_state=100) \u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m out \u001b[39m=\u001b[39m ph\u001b[39m.\u001b[39;49mparse(data[\u001b[39m\"\u001b[39;49m\u001b[39mtext_clean\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mvalues)\n\u001b[1;32m    354\u001b[0m out \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(out)\n\u001b[1;32m    355\u001b[0m data[\u001b[39m\"\u001b[39m\u001b[39mtext_y\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m out[\u001b[39m\"\u001b[39m\u001b[39mtext_y\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mvalues\n",
      "File \u001b[0;32m/mnt/c/Users/xif626/Documents/GitHub/weibo_sentiment/weibo_sentiment/bert_senta/predict2.py:108\u001b[0m, in \u001b[0;36mparse_handler.parse\u001b[0;34m(self, text_list)\u001b[0m\n\u001b[1;32m    106\u001b[0m result \u001b[39m=\u001b[39m []\n\u001b[1;32m    107\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mload samples\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m test_examples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocessor\u001b[39m.\u001b[39;49mget_ifrn_examples(text_list)\n\u001b[1;32m    109\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mload fetures\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    110\u001b[0m test_features \u001b[39m=\u001b[39m convert_examples_to_features(\n\u001b[1;32m    111\u001b[0m     test_examples, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_list, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mmax_seq_length, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer, show_exp\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/c/Users/xif626/Documents/GitHub/weibo_sentiment/weibo_sentiment/bert_senta/train.py:115\u001b[0m, in \u001b[0;36mMyPro.get_ifrn_examples\u001b[0;34m(self, text_list)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_ifrn_examples\u001b[39m(\u001b[39mself\u001b[39m, text_list):\n\u001b[0;32m--> 115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_ifrn_examples(text_list, \u001b[39m\"\u001b[39;49m\u001b[39mifrn\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m/mnt/c/Users/xif626/Documents/GitHub/weibo_sentiment/weibo_sentiment/bert_senta/train.py:125\u001b[0m, in \u001b[0;36mMyPro._create_ifrn_examples\u001b[0;34m(self, text_list, set_type)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[39mfor\u001b[39;00m (i, text) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(text_list):\n\u001b[1;32m    123\u001b[0m     guid \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (set_type, i)\n\u001b[1;32m    124\u001b[0m     examples\u001b[39m.\u001b[39mappend(\n\u001b[0;32m--> 125\u001b[0m         InputExample(guid\u001b[39m=\u001b[39mguid, text_a\u001b[39m=\u001b[39mtext\u001b[39m.\u001b[39;49mstrip()))\n\u001b[1;32m    126\u001b[0m \u001b[39mreturn\u001b[39;00m examples\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "run predict2.py --input_path \"/mnt/g/weibo_data/weibo_mit_parquet/\" --output_path \"/mnt/g/weibo_data/weibo_mit_sentiment\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有的都成功了就只有這個7hao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_7 = pd.read_parquet(\"/mnt/g/weibo_data/weibo_mit_parquet/Historical_Geotagged_Data_7.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6 = pd.read_parquet(\"/mnt/g/weibo_data/weibo_mit_parquet/Historical_Geotagged_Data_6.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000482, 19), (999062, 19), (1000482, 19))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_7.dropna(subset=[\"text\"]).shape, df_7.dropna(subset=[\"text_clean\"]).shape,df_7.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看起來還是需要進行去除None的處理啊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7 = df_7.dropna(subset=[\"text_clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_7.to_parquet(\"/mnt/g/weibo_data/weibo_mit_parquet/Historical_Geotagged_Data_7_drop_text_na.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/08/2023 17:12:10 - INFO - __main__ -   device cuda n_gpu 1 distributed training False\n",
      "02/08/2023 17:12:10 - WARNING - pytorch_pretrained_bert.tokenization -   The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Init model started.\n",
      "using cuda Quadro P2200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/08/2023 17:12:10 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese-vocab.txt from cache at /home/kang/.pytorch_pretrained_bert/8a0c070123c1f794c42a29c6904beb7c1b8715741e235bee04aca2c7636fc83f.9b42061518a39ca00b8b52059fd2bede8daa613f8a8671500e518a8c29de8c00\n",
      "02/08/2023 17:12:10 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz from cache at /home/kang/.pytorch_pretrained_bert/distributed_-1/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f\n",
      "02/08/2023 17:12:10 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file /home/kang/.pytorch_pretrained_bert/distributed_-1/42d4a64dda3243ffeca7ec268d5544122e67d9d06b971608796b483925716512.02ac7d664cff08d793eb00d6aac1d04368a1322435e5fe0a27c70b0b3a85327f to temp dir /tmp/tmp4kthfqo0\n",
      "02/08/2023 17:12:14 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n",
      "02/08/2023 17:12:16 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "02/08/2023 17:12:16 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]Init model finished.\n",
      "The following 1 data will be processed\n",
      "                                          file_name\n",
      "0  Historical_Geotagged_Data_7_drop_text_na.parquet\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3930392718d4631b83b4ef6442a36f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "total:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Historical_Geotagged_Data_7_drop_text_na.parquet\n",
      "load samples\n",
      "load fetures\n",
      "load train data\n",
      "load tensor\n",
      "Run prediction for full data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c76b0b58fbc4ec29dfe8daedd0414ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "predicting:   0%|          | 0/999062 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run predict2.py --input_path \"/mnt/g/weibo_data/weibo_mit_parquet_drop_na/\" --output_path \"/mnt/g/weibo_data/weibo_mit_sentiment\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to daily files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "input_path = \"/mnt/g/weibo_data/weibo_mit_sentiment/\"\n",
    "output_path = \"/mnt/g/weibo_data/weibo_mit_sentiment_daily/\"\n",
    "data_list = []\n",
    "for file_name in os.listdir(input_path):\n",
    "    df = pd.read_parquet(os.path.join(input_path, file_name))\n",
    "    data_list.append(df)\n",
    "data_list = pd.concat(data_list, axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert created_at to datetime\n",
    "data_list[\"created_at\"] = pd.to_datetime(data_list[\"created_at\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert isLongText to bool\n",
    "data_list[\"isLongText\"] = data_list[\"isLongText\"].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list.to_parquet(os.path.join(output_path, \"weibo_mit_sentiment.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the data into daily data in the output_path with parquet format\n",
    "data_list.groupby(data_list[\"created_at\"].dt.date).apply(lambda x: x.to_parquet(\n",
    "    os.path.join(output_path, str(x[\"created_at\"].dt.date.iloc[0]) + \".parquet\")))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spatial join the data with the shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GbCity</th>\n",
       "      <th>City_EN</th>\n",
       "      <th>GbProv</th>\n",
       "      <th>Prov_EN</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1100</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>POLYGON ((116.68522 41.02011, 116.66689 40.976...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1200</td>\n",
       "      <td>Tianjin</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Tianjin</td>\n",
       "      <td>POLYGON ((117.46381 40.24003, 117.49619 40.227...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1301</td>\n",
       "      <td>Shijiazhuang</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Hebei</td>\n",
       "      <td>POLYGON ((113.79636 38.76263, 113.82275 38.755...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1302</td>\n",
       "      <td>Tangshan</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Hebei</td>\n",
       "      <td>MULTIPOLYGON (((118.55785 38.96500, 118.55545 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1303</td>\n",
       "      <td>Qinhuangdao</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Hebei</td>\n",
       "      <td>POLYGON ((119.84830 40.03310, 119.83793 39.990...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  GbCity       City_EN  GbProv  Prov_EN  \\\n",
       "0   1100       Beijing    11.0  Beijing   \n",
       "1   1200       Tianjin    12.0  Tianjin   \n",
       "2   1301  Shijiazhuang    13.0    Hebei   \n",
       "3   1302      Tangshan    13.0    Hebei   \n",
       "4   1303   Qinhuangdao    13.0    Hebei   \n",
       "\n",
       "                                            geometry  \n",
       "0  POLYGON ((116.68522 41.02011, 116.66689 40.976...  \n",
       "1  POLYGON ((117.46381 40.24003, 117.49619 40.227...  \n",
       "2  POLYGON ((113.79636 38.76263, 113.82275 38.755...  \n",
       "3  MULTIPOLYGON (((118.55785 38.96500, 118.55545 ...  \n",
       "4  POLYGON ((119.84830 40.03310, 119.83793 39.990...  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geopandas as gpd\n",
    "china_base_map = gpd.read_file(\"/mnt/g/work/weibo_covid_19/data/raw/china_city_basemap.zip\")\n",
    "china_base_map = china_base_map[['GbCity', 'City_EN', 'GbProv', 'Prov_EN',  'geometry']]\n",
    "# china_base_map.columns\n",
    "china_base_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = gpd.GeoDataFrame(data_list, \n",
    "        geometry=gpd.points_from_xy(data_list[\"longitude\"], data_list[\"latitude\"]),\n",
    "        crs=\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the data with the basemap\n",
    "data_joined = gpd.sjoin(data_list, china_base_map, how=\"left\", predicate=\"intersects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9954285, 24), (9954791, 29))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list.shape, data_joined.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "may some data is out of China"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9954285, 24), (9954791, 29))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list.shape, data_joined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_joined[\"date\"] = data_joined[\"created_at\"].dt.date\n",
    "df = data_joined[['GbCity', 'City_EN', 'GbProv', 'Prov_EN', 'sentiment_scores_1', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_joined.to_parquet(\"/mnt/g/weibo_data/weibo_mit_sentiment_with_cities.parquet\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create daily mean sentiment for each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create daily mean sentiment for each city\n",
    "\n",
    "df_st = df.groupby([\"date\", \"GbCity\", \"City_EN\", \"GbProv\", \"Prov_EN\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>GbCity</th>\n",
       "      <th>City_EN</th>\n",
       "      <th>GbProv</th>\n",
       "      <th>Prov_EN</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th>GbCity</th>\n",
       "      <th>City_EN</th>\n",
       "      <th>GbProv</th>\n",
       "      <th>Prov_EN</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2018-01-01</th>\n",
       "      <th>1100</th>\n",
       "      <th>Beijing</th>\n",
       "      <th>11.0</th>\n",
       "      <th>Beijing</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1100</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Beijing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <th>Tianjin</th>\n",
       "      <th>12.0</th>\n",
       "      <th>Tianjin</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1200</td>\n",
       "      <td>Tianjin</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Tianjin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <th>Shijiazhuang</th>\n",
       "      <th>13.0</th>\n",
       "      <th>Hebei</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1301</td>\n",
       "      <td>Shijiazhuang</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Hebei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <th>Tangshan</th>\n",
       "      <th>13.0</th>\n",
       "      <th>Hebei</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1302</td>\n",
       "      <td>Tangshan</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Hebei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1303</th>\n",
       "      <th>Qinhuangdao</th>\n",
       "      <th>13.0</th>\n",
       "      <th>Hebei</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1303</td>\n",
       "      <td>Qinhuangdao</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Hebei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2021-07-28</th>\n",
       "      <th>3607</th>\n",
       "      <th>Ganzhou</th>\n",
       "      <th>36.0</th>\n",
       "      <th>Jiangxi</th>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>3607</td>\n",
       "      <td>Ganzhou</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Jiangxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4301</th>\n",
       "      <th>Changsha</th>\n",
       "      <th>43.0</th>\n",
       "      <th>Hunan</th>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>4301</td>\n",
       "      <td>Changsha</td>\n",
       "      <td>43.0</td>\n",
       "      <td>Hunan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5101</th>\n",
       "      <th>Chengdu</th>\n",
       "      <th>51.0</th>\n",
       "      <th>Sichuan</th>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>5101</td>\n",
       "      <td>Chengdu</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Sichuan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5133</th>\n",
       "      <th>Ganzicangzu</th>\n",
       "      <th>51.0</th>\n",
       "      <th>Sichuan</th>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>5133</td>\n",
       "      <td>Ganzicangzu</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Sichuan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5332</th>\n",
       "      <th>Lijiangdiqu</th>\n",
       "      <th>53.0</th>\n",
       "      <th>Yunnan</th>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>5332</td>\n",
       "      <td>Lijiangdiqu</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Yunnan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>412235 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     date GbCity  \\\n",
       "date       GbCity City_EN      GbProv Prov_EN                      \n",
       "2018-01-01 1100   Beijing      11.0   Beijing  2018-01-01   1100   \n",
       "           1200   Tianjin      12.0   Tianjin  2018-01-01   1200   \n",
       "           1301   Shijiazhuang 13.0   Hebei    2018-01-01   1301   \n",
       "           1302   Tangshan     13.0   Hebei    2018-01-01   1302   \n",
       "           1303   Qinhuangdao  13.0   Hebei    2018-01-01   1303   \n",
       "...                                                   ...    ...   \n",
       "2021-07-28 3607   Ganzhou      36.0   Jiangxi  2021-07-28   3607   \n",
       "           4301   Changsha     43.0   Hunan    2021-07-28   4301   \n",
       "           5101   Chengdu      51.0   Sichuan  2021-07-28   5101   \n",
       "           5133   Ganzicangzu  51.0   Sichuan  2021-07-28   5133   \n",
       "           5332   Lijiangdiqu  53.0   Yunnan   2021-07-28   5332   \n",
       "\n",
       "                                                    City_EN  GbProv  Prov_EN  \n",
       "date       GbCity City_EN      GbProv Prov_EN                                 \n",
       "2018-01-01 1100   Beijing      11.0   Beijing       Beijing    11.0  Beijing  \n",
       "           1200   Tianjin      12.0   Tianjin       Tianjin    12.0  Tianjin  \n",
       "           1301   Shijiazhuang 13.0   Hebei    Shijiazhuang    13.0    Hebei  \n",
       "           1302   Tangshan     13.0   Hebei        Tangshan    13.0    Hebei  \n",
       "           1303   Qinhuangdao  13.0   Hebei     Qinhuangdao    13.0    Hebei  \n",
       "...                                                     ...     ...      ...  \n",
       "2021-07-28 3607   Ganzhou      36.0   Jiangxi       Ganzhou    36.0  Jiangxi  \n",
       "           4301   Changsha     43.0   Hunan        Changsha    43.0    Hunan  \n",
       "           5101   Chengdu      51.0   Sichuan       Chengdu    51.0  Sichuan  \n",
       "           5133   Ganzicangzu  51.0   Sichuan   Ganzicangzu    51.0  Sichuan  \n",
       "           5332   Lijiangdiqu  53.0   Yunnan    Lijiangdiqu    53.0   Yunnan  \n",
       "\n",
       "[412235 rows x 5 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df_st.index.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st[[\"date\", \"GbCity\", \"City_EN\", \"GbProv\", \"Prov_EN\"]] = df_st.index.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment_scores_1</th>\n",
       "      <th>date</th>\n",
       "      <th>GbCity</th>\n",
       "      <th>City_EN</th>\n",
       "      <th>GbProv</th>\n",
       "      <th>Prov_EN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.831727</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1100</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>11.0</td>\n",
       "      <td>Beijing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.809378</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1200</td>\n",
       "      <td>Tianjin</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Tianjin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.804597</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1301</td>\n",
       "      <td>Shijiazhuang</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Hebei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.860097</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1302</td>\n",
       "      <td>Tangshan</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Hebei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.842769</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>1303</td>\n",
       "      <td>Qinhuangdao</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Hebei</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412230</th>\n",
       "      <td>0.974551</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>3607</td>\n",
       "      <td>Ganzhou</td>\n",
       "      <td>36.0</td>\n",
       "      <td>Jiangxi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412231</th>\n",
       "      <td>0.978843</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>4301</td>\n",
       "      <td>Changsha</td>\n",
       "      <td>43.0</td>\n",
       "      <td>Hunan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412232</th>\n",
       "      <td>0.994251</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>5101</td>\n",
       "      <td>Chengdu</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Sichuan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412233</th>\n",
       "      <td>0.975517</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>5133</td>\n",
       "      <td>Ganzicangzu</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Sichuan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412234</th>\n",
       "      <td>0.999921</td>\n",
       "      <td>2021-07-28</td>\n",
       "      <td>5332</td>\n",
       "      <td>Lijiangdiqu</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Yunnan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>412235 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment_scores_1        date GbCity       City_EN  GbProv  Prov_EN\n",
       "0                 0.831727  2018-01-01   1100       Beijing    11.0  Beijing\n",
       "1                 0.809378  2018-01-01   1200       Tianjin    12.0  Tianjin\n",
       "2                 0.804597  2018-01-01   1301  Shijiazhuang    13.0    Hebei\n",
       "3                 0.860097  2018-01-01   1302      Tangshan    13.0    Hebei\n",
       "4                 0.842769  2018-01-01   1303   Qinhuangdao    13.0    Hebei\n",
       "...                    ...         ...    ...           ...     ...      ...\n",
       "412230            0.974551  2021-07-28   3607       Ganzhou    36.0  Jiangxi\n",
       "412231            0.978843  2021-07-28   4301      Changsha    43.0    Hunan\n",
       "412232            0.994251  2021-07-28   5101       Chengdu    51.0  Sichuan\n",
       "412233            0.975517  2021-07-28   5133   Ganzicangzu    51.0  Sichuan\n",
       "412234            0.999921  2021-07-28   5332   Lijiangdiqu    53.0   Yunnan\n",
       "\n",
       "[412235 rows x 6 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_st = df_st.reset_index(drop=True)\n",
    "df_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.to_parquet(\"/mnt/g/weibo_data/weibo_mit_sentiment_with_cities_daily_mean.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st.to_csv(\"/mnt/g/weibo_data/weibo_mit_sentiment_with_cities_daily_mean.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weibo_process",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff270d1f9216c153058d9acbe9499c81ebfef40ca0541438cf51cb45ccfeedd4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
